* 6-2. 기울기 기반 학습

* 신경망은 비선형성이기 때문에 볼록함수를 손실함수로 사용하기가 어려움

* 비볼록 손실함수에 확률적 경사하강법을 적용할 때는 수렴을 보장할 수 없으며, 결과가 초깃값에 민감하게 변한다.

## 비용함수

* 비용함수의 선택은 심층 신경망 설계의 중요한 측면 중 하나

### 최대가능도를 이용한 조건부 확률 학습

* 대다수의 경우 최대가능도를 사용해서 훈련하는데 로그가능도는 교체 엔트로피로도 동일하게 서술이 가능함

* 최대가능도 비용함수 유도는 모형에 따라 비용함수가 자동으로 결정된다는 장점을 가지고 있음

* 인수가 아주 큰 음수일 때 포화하는 지수함수가 출력 단위에 관여하는 경우가 있는데 이 경우 비용함수의 로그가 상쇄시킬 수 있음

### 조건부 통계량의 학습

* 모형이 전체 확률분포를 배우는 것이 아니라 하나의 조건부 통계량(평균과 같은)만 배우면 되는 경우도 있음

* 비용함수를 범함수로 간주하면, 학습이라는 것을 특정 형태의 매개변수 구성을 선택하는 것이 아니라 함수 자체를 선택하는 것으로 생각 가능
    * 범함수(functional): 함수에서 실수로 mapping

* 평균제곱오차, 평균절대오차를 기울기 기반 최적화와 함께 사용하면 성능이 나쁠 때가 많다. 교차 엔트로피를 사용하는 것이 더 좋은 결과를 가져온다.

## 출력 단위

### 가우스 출력 분포를 위한 선형 단위

* 선형 단위는 조건부 가우스 분포의 평균을 산출하는 용도로 흔히 쓰이며 이 경우 로그가능도를 최대화하는 것은 평균제곱오차를 최소화하는 것과 동등하다.

### 베르누이 출력 분포를 위한 S자형 단위

* 이진 변수를 예측하는 문제들에 최대가능도 접근 방식을 적용할 때는 베르누이 분포를 적용

* 이 경우 출력층이 선형 단위 하나로 이루어지고 그 단위의 값이 유효한 확률이 되게 하려면 S자형(sigmoid) 출력 단위를 최대가능도와 결합하는 접근 방식이 필요함

### 멀티누이 출력 분포를 위한 소프트맥스 단위

* 가능한 값이 n가지인 이산 변수에 관한 확률분포를 표현해야 할 때는 소프트맥스 함수를 사용하면 된다.
    * 소프트맥스 함수는 분류기의 출력 단위에서 서로 다른 n가지 부류에 관한 확률분포를 나타내는 용도로 쓰일 때가 아주 많다.

* 로그가능도 이외의 목적함수들은 소프트맥스 함수와 잘 맞지 않을 때가 많다.

### 그 외의 출력 단위

* 가장 흔히 쓰이는 출력 단위는 지금까지 설명한 선형, S자형, 소프트맥스 출력 단위이다. 신경망들은 우리가 원하는 그 어떤 종류의 출력층으로도 일반화할 수 있다. 그리고 거의 모든 종류의 출력층에 대해, 최대가능도 원리는 좋은 비용함수를 설계하는 지침이 된다.

* 일반화하자면 조건부 확률분포 P(y|x;θ)를 정의했다고 할 때, 최대가능도 원리가 제시하는 비용함수는 -logP(y|x;θ)이다.
