# 6-3. 은닉 단위

## 정류 선형 단위와 그 일반화들

* 정류 선형 단위(Rectified Linear Unit, ReLU)는 활성화 함수 g(z)=max(0, z)를 사용

* 기울기 방향이ㅐ 2차 효과들을 도입하는 활성화 함수의 경우 보다 학습에 훨씬 더 유리하다.

* 정류 선형 단위의 일반화

1. 절댓값 정류
2. 누출 ReLU
3. 매개변수적 ReLU
4. 맥스아웃 단위

* 정류 선형 단위와 그것의 모든 일반화는 모형의 행동 방식이 선형 모형에 가까울수록 최적화하기 쉽다는 원리에 기초한다.

## 로그 S자형 단위와 쌍곡탄젠트 단위

* 정류 선형 단위가 등장하기 전에는 로그 S자형 활성화 함수와 쌍곡탄젠트 활성화 함수를 사용했음

* 포화 범위가 넓어서 기울기 기반 학습을 진행하기가 어려움
    * 출력 단위에서는 적절한 비용 함수와 함께 사용할 수 있음

* S자형 활성화 함수를 반드시 사용해야 한다면, 쌍곡탄젠트 활성화 함수가 로그 S자형 함수보다 나은 성과를 보일 때가 많다.

* 순환 신경망과 여러 확률적 모형들, 그리고 몇몇 자동부호기들은 추가적인 요구조건 때문에 조각별 선형 활성화 함수를 사용할 수 없다. 그런 모형들에서는 S자형 단위가 더 효과적이다.

## 기타 은닉 단위들

* 기타 많은 종류의 은닉 단위가 있지만 이미 알려진 종류의 은닉 단위와 대략 비슷한 성과를 내는 일은 흔하기 때문에 흥미롭지 않다.

### 특별히 유용한 은닉 단위들

1. 선형 함수
2. 소프트맥스 함수
3. 방사상 기저함수
4. 소프트플러스
5. 유계 쌍곡탄젠트
