# 3-13. 정보 이론

## 정보 이론

* 유래: 잡음 섞인 통신 채널로 이산적인 알파벳들을 전송해서 메시지를 보내는 문제를 연구하기 위해 만듦

* 정보 이론에 깔린 기본적인 직관: 발생 가능성이 낮은 사건을 배우는 것이 발생 가능성이 큰 사건을 배우는 것보다 더 많은 정보를 얻을 수 있다는 점

    * "오늘 아침에 해가 떴다."라는 정보보다 "오늘 아침에 일식이 있었다."는 메시지가 담은 정보의 양이 더 많음

### 정보를 수량화하기

* 발생 가능성이 큰 사건은 정보량이 적어야 한다. 극단적인 경우, 반드시 발생하는 사건에는 아무런 정보도 없어야 한다.
* 발생 가능성이 낮은 사건은 정보량이 많아야 한다.
* 개별 사건들의 정보량을 더할 수 있어야 한다. 예를 들어, 동전을 두 번 던져서 두 번 다 앞면이 나온 사건의 정보량은 동전을 한 번 던져서 앞면이 나온 사건의 정보량의 두 배이어야 한다.
* 위 세 조건을 충족하기 위해 사건 x = x의 자기 정보(self-infomation)를 `I(x) = -logP(x)` 로 정의한다.

    * 단위는 자연로의 경우 내트(nat), 기수 2 로그의 경우 비트 또는 섀넌(shannon)

    * 확률밀도가 1인 사건이라고 해서 반드시 발생하지는 않는다. 하지만 그 사건의 정보량은 0이 된다.

#### 섀년 엔트로피

* 엔트로피(entrophy): -∑P(x)*logP(x)
    * 분포 P에서 뽑은 기호들을 부호화하는 데 필요한 평균 비트 수의 하계(lower bound)
    * 일종의 `정보량`

#### Cross-Entrophy

* H(P, Q) = -∑P(x)*logQ(x)
    * P(x), Q(x)의 차이가 크면 값이 커짐

#### 쿨백-라이블러 발산값(Kullback-Leibler divergence, KL Divergence)

* H(P, Q) - H(P)
    * P, Q가 서로 다르면 엔트로피가 얼마나 높아질까?
    * 음수가 될 수 없음
    * 비대칭이기 때문에 P, Q의 순서가 학습 결과에 영향을 줌
