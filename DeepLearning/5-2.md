# 5-2. 수용력, 과대적합, 과소적합

```
기계 학습의 주된 어려움은 모형을 훈련하는 데 사용한 입력뿐만 아니라 새로운, 이전에 해본 적 없는 입력에 대해서도 알고리즘이 잘 작동하게 만드는 것이다.
```

* 일반화(generalization): 이전에 관측한 적이 없는 입력들에 대해 잘 작동하는 능력
* 훈련 오차(training error): 예측한 값과 훈련 집합에 있는 참값 사이의 오차
* 일반화 오차(generalization error): 새 입력에 대한 오차의 기댓값

## 통계적 확률 이론

* 훈련 집합만 관측할 수 있는 상황에서 시험 집합에 대한 성과를 개선하는 것이 가능할까? -> 훈련 집합과 시험 집합을 수집하는 데 일정한 가정, 제약을 두면 개선이 가능
* 자료 생성 과정(data-generating process, DGP): 각 자료 집합의 견본들이 서로 독립이고 훈련 집합과 자료 집합의 견본들이 같은 확률 분포에 따라 동일하게 분포되어 있다는 가정
* 자료 생성 분포(data-generating distribution): 자료 생성 과정의 확률분포를 사용해서 모든 훈련과 시험 견본을 생성할 수 있는데, 이 떄의 확률분포를 자료 생성 분포라 부름

```
무작위로 선택된 모형의 기대 훈련 오차는 그 모형의 기대 시험 오차와 같다.
```
