# 5-2. 수용력, 과대적합, 과소적합

```
기계 학습의 주된 어려움은 모형을 훈련하는 데 사용한 입력뿐만 아니라 새로운, 이전에 해본 적 없는 입력에 대해서도 알고리즘이 잘 작동하게 만드는 것이다.
```

* 일반화(generalization): 이전에 관측한 적이 없는 입력들에 대해 잘 작동하는 능력
* 훈련 오차(training error): 예측한 값과 훈련 집합에 있는 참값 사이의 오차
* 일반화 오차(generalization error): 새 입력에 대한 오차의 기댓값

### 통계적 확률 이론

* 훈련 집합만 관측할 수 있는 상황에서 시험 집합에 대한 성과를 개선하는 것이 가능할까? -> 훈련 집합과 시험 집합을 수집하는 데 일정한 가정, 제약을 두면 개선이 가능
* 자료 생성 과정(data-generating process, DGP): 각 자료 집합의 견본들이 서로 독립이고 훈련 집합과 자료 집합의 견본들이 같은 확률 분포에 따라 동일하게 분포되어 있다는 가정
* 자료 생성 분포(data-generating distribution): 자료 생성 과정의 확률분포를 사용해서 모든 훈련과 시험 견본을 생성할 수 있는데, 이 떄의 확률분포를 자료 생성 분포라 부름

```
무작위로 선택된 모형의 기대 훈련 오차는 그 모형의 기대 시험 오차와 같다.
```

### 기계 학습 알고리즘의 성과

1. 훈련 오차를 작게 만드는 능력(underfitting)
2. 훈련 오차와 시험 오차의 차이를 작게 만드는 능력(overfitting)

* 수용력(capacity): 학습 모형이 다양한 함수들에 적합하는 능력
    * 알고리즘의 가설 공간을 적절히 선택함으로써 모형의 수용력을 더 크게 할 수 있음
    * 최적 수용력 모형의 경우 시험 오차는 베이즈 오차에 점근함

## 공짜 점심 없음 정리

* 귀납 추론의 한계: 주어진 집합의 모든 구성원을 설명하는 법칙을 추론하려면 반드시 그 집합의 모든 구성원에 대한 정보가 필요
    * 기계 학습은 확률론을 도입함으로서 부분적으로 피해갈 수 있음

```
기계 학습은 거의 대부분의 구성원에 대해 정확할 가능성이 있는 규칙들을 찾고자 한다.
```

### 공짜 점심 없음 정리

* 모든 가능한 자료 생성 분포에 대해 평균을 구한다고 할 때, 이전에 관측한 적이 없는 자료점들을 분류하는 과제에서 모든 분류 알고리즘의 오차율은 서로 같다.
    * 보편적인 학습 알고리즘은 없다.

* 실제 응용에서 마주치는 확률분포의 종류에 일정한 제약을 두면 잘 작동하는 학습 알고리즘을 구현할 수 있다.
